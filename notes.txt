https://github.com/mistralai/mistral-finetune?tab=readme-ov-file
# https://colab.research.google.com/github/mistralai/mistral-finetune/blob/main/tutorials/mistral_finetune_7b.ipynb



ERROR: Could not find a version that satisfies the requirement triton==2.2 (from versions: none)
ERROR: No matching distribution found for triton==2.2
(finetuning-demo) PS \mistral-finetune> pip install Triton-windows


# weird output related
# https://github.com/unslothai/unsloth/issues/877
# https://github.com/unslothai/unsloth/issues/787


For eval:
https://docs.unsloth.ai/basics/troubleshooting-and-faqs#how-to-do-evaluation

100gb vol for converting llama.cpp


https://huggingface.co/docs/datasets/en/load_hub

Just use vscode-remote-ssh it's easier

apply the LoRA adapter using model.load_adapter or FastLanguageModel.get_peft_model, 
or merge the adapter into the base model and save it as a merged model for future direct loading.


Data flow:
- Plain text
    - Chunk with docling, then contextualize
    - use llm to generate context
    -> w .json
- r .json
    - Append context into question
    -> w instruction.json
Load the output checkpoint as llm



if can run finetune but it breaks after like 10 steps, need to reduce seq length
Unsupported conversion from f16 to f16 LLVM ERROR: Unsupported rounding mode for conversion.


https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb
- input examples, sharegpt, completion

# Both works, on hf and downloaded locally
dataset = load_dataset("json", data_files="/home/ubuntu/finetuning-demo/src/data/dataset.jsonl", split = "train")
dataset = load_dataset("pookie3000/donald_trump_interviews", split="train")


================
# same data
loss for mistral-7b-instruct
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.8824, 'grad_norm': 9.151384353637695, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 1.0191, 'grad_norm': nan, 'learning_rate': 4e-05, 'epoch': 0.01}
{'loss': 0.9445, 'grad_norm': nan, 'learning_rate': 4e-05, 'epoch': 0.01}
  1%|█▏    

loss for meta-llama-3.1-8b
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 2.9963, 'grad_norm': 1.5264372825622559, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 2.9777, 'grad_norm': 1.6279881000518799, 'learning_rate': 4e-05, 'epoch': 0.01}
  1%|▊       

loss for mistral-7b 
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.3174, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 1.4853, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.01}
{'loss': 1.155, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.01}
{'loss': 2.1475, 'grad_norm': inf, 'learning_rate': 0.0, 'epoch': 0.02}
{'loss': 1.6843, 'grad_norm': 13.035565376281738, 'learning_rate': 0.0, 'epoch': 0.02}
{'loss': 1.3547, 'grad_norm': 15.03381633758545, 'learning_rate': 4e-05, 'epoch': 0.03}
{'loss': 1.2782, 'grad_norm': 11.31531810760498, 'learning_rate': 8e-05, 'epoch': 0.03}
{'loss': 1.0215, 'grad_norm': 13.942950248718262, 'learning_rate': 0.00012, 'epoch': 0.04}
{'loss': 0.4631, 'grad_norm': 6.264810562133789, 'learning_rate': 0.00016, 'epoch': 0.04}
{'loss': 0.0652, 'grad_norm': 4.181741714477539, 'learning_rate': 0.0002, 'epoch': 0.05}


...

Further dataset notes:
https://github.com/unslothai/unsloth/issues/337
- how do i make a Dataset correctly? #337
https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb#scrollTo=LjY75GoYUCB8
https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing#scrollTo=95_Nn-89DhsL
- alpaca + mistral7b
https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing#scrollTo=buRvIVj1UlSN
- chatML and chat templates for mistral7b
https://www.reddit.com/r/unsloth/comments/1j83car/new_finetuning_101_guide_for_best_practices_basics/
- fine tuning guide and best practices... actual unsloth doc
    https://docs.unsloth.ai/get-started/fine-tuning-llms-guide
    https://docs.unsloth.ai/basics/datasets-guide
https://github.com/JayGLXR/SynthDataGen
- Synthetic data creation (rust)



{"conversations": [{"role": "user", "content": "content here"}, {"role": "assistant", "content": "content here"}]}
{"conversations": [{"role": "user", "content": "content here"}, {"role": "assistant", "content": "content here"}]}


data validation code output
- run snippet to see if any runtime errors
- see if A in Q&A has any weird tokens
    - check if syntax going in is good
- use synthetic generation for Q's
- look for best practices in generated code validation
-==> quantitive (metrics with wandb) and qualitative (??)
- https://platform.openai.com/docs/guides/evals?api-mode=responses

Use benchmark data from HF