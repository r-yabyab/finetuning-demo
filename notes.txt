https://github.com/mistralai/mistral-finetune?tab=readme-ov-file
# https://colab.research.google.com/github/mistralai/mistral-finetune/blob/main/tutorials/mistral_finetune_7b.ipynb



ERROR: Could not find a version that satisfies the requirement triton==2.2 (from versions: none)
ERROR: No matching distribution found for triton==2.2
(finetuning-demo) PS \mistral-finetune> pip install Triton-windows


# weird output related
# https://github.com/unslothai/unsloth/issues/877
# https://github.com/unslothai/unsloth/issues/787


For eval:
https://docs.unsloth.ai/basics/troubleshooting-and-faqs#how-to-do-evaluation

100gb vol for converting llama.cpp


https://huggingface.co/docs/datasets/en/load_hub

Just use vscode-remote-ssh it's easier

apply the LoRA adapter using model.load_adapter or FastLanguageModel.get_peft_model, 
or merge the adapter into the base model and save it as a merged model for future direct loading.


Data flow:
- Plain text
    - Chunk with docling, then contextualize
    - use llm to generate context
    -> w .json
- r .json
    - Append context into question
    -> w instruction.json